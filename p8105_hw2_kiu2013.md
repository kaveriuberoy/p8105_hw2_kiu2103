p8105_hw2_kiu2103
================
Kaveri Uberoy
2025-09-25

Loading in packages we need.

``` r
library(tidyverse)
```

    ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
    ## ✔ dplyr     1.1.4     ✔ readr     2.1.5
    ## ✔ forcats   1.0.0     ✔ stringr   1.5.1
    ## ✔ ggplot2   3.5.2     ✔ tibble    3.3.0
    ## ✔ lubridate 1.9.4     ✔ tidyr     1.3.1
    ## ✔ purrr     1.1.0     
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()
    ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

``` r
library(readxl)
library(dplyr)
library(knitr)
```

## Problem 1

Our goal is to merge these into a single data frame using year and month
as keys across datasets.

Cleaning the data in pols-month.csv. Used prez_dem value to create
president variable instead of prez_gop value as prez_gop value is
sometimes 1 and sometimes 2.

``` r
pols_df = 
  read_csv("data/pols-month.csv") |>
  janitor::clean_names() |>
  separate(mon, into = c("year", "month", "day"), sep = "-") |>
  mutate(across(c(year, month, day), as.integer)) |>
  mutate(month = month.name[month]) |>
  mutate(president = if_else(prez_dem == 1, "dem" , "gop")) |>
  select(-prez_gop, -prez_dem) |>
  select(-day)
```

    ## Rows: 822 Columns: 9
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl  (8): prez_gop, gov_gop, sen_gop, rep_gop, prez_dem, gov_dem, sen_dem, r...
    ## date (1): mon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

Clean the data in snp.csv using a similar process to the above. Also
removed the day variable here to match pols_df tidying, made changes to
year value to also match pols_df.

``` r
snp_df = 
  read_csv("data/snp.csv") |>
  janitor::clean_names() |>
  separate(date, into = c("month", "day", "year"), sep = "/") |>
  mutate(across(c(year, month, day), as.integer)) |>
  mutate(year = if_else(year > 25, 1900 + year, 2000 + year)) |>
  mutate(month = month.name[month]) |>
  select(-day) |>
  relocate(year, month)
```

    ## Rows: 787 Columns: 2
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (1): date
    ## dbl (1): close
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

Tidying the unemployment data so that it can be merged with the previous
datasets by month and year.

``` r
unemployment_df =
  read_csv("data/unemployment.csv") |>
  janitor::clean_names() |>
  pivot_longer(
    cols = -year,
    names_to = "month",
    values_to = "unemployment_rate"
  ) |>
  mutate(
    month_num = match(tolower(month), tolower(month.abb)),
    month = month.name[month_num] 
  ) |>
  select(year, month, unemployment_rate)
```

    ## Rows: 68 Columns: 13
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (13): Year, Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

Join the datasets by left joining snp into pols, and left joining
unemployment into the result.

``` r
pol_eco_fin_merged_df =
  pols_df |>
  left_join(snp_df, by = c("year", "month")) |>
  left_join(unemployment_df, by = c("year", "month"))
```

Write a short paragraph about these datasets. Explain briefly what each
dataset contained, and describe the resulting dataset (e.g. give the
dimension, range of years, and names of key variables).

The dataset construction involved merging three different csv files
containing political, economic, and financial data into one tidy
dataset, The pols\_-month_df dataset includes monthly counts of
Democratic and Republican governors, senators, and representatives,
along with the party of the president. The mon date variable was split
into year and month, and a new president variable was created using the
value of the prez_dem (if equal to 1, president column = dem, if equal
to zero then gop). The snp_df dataset provides the Standard & Poor’s
stock market index (S&P) closing values for specific dates. The date
column was transformed into year and month so that we could merge with
the other datasets later. The unemployment_df dataset, which contains
unemployment rates for each date (month/year) originally in a wide
format with months as column headers, was pivoted to a long format for
compatibility. Month abbreviations were converted to full names to align
with the other datasets for merging as well. The resulting dataset,
pol_eco_fin_merged_df (left joined by date values) combines all three
sources and includes 822 observations with 11 variables like year,
month, president, counts of politicians by party, close (S&P 500 index),
and unemployment. It spans a range of years (January 1947 to June 2015)
from the earliest in pols_df to the latest in snp_df (depending on
overlap). Key variables include president, sen_gop, rep_dem, close, and
unemployment. The dataset is useful to look at politics, economics, and
unemployment data together to compare these different factors.

## Problem 2

Reading and cleaning the Mr. Trash Wheel sheet:

``` r
trashwheel_df =
  read_excel("data/trash_wheel_collection_data.xlsx", 
             sheet = "Mr. Trash Wheel",
             range = "A2:N709") |>
  janitor::clean_names() |>
  mutate(
      sports_balls = as.integer(round(sports_balls)),
      year = as.numeric(year)
    )
```

Importing, cleaning, and organizing the data for Professor Trash Wheel
and Gwynnda, and combining this with the Mr. Trash Wheel dataset to
produce a single tidy dataset.

``` r
professor_trashwheel_df =
  read_excel("data/trash_wheel_collection_data.xlsx", 
             sheet = "Professor Trash Wheel",
             range = "A2:M134") |>
  janitor::clean_names()

gwynnda_trashwheel_df =
  read_excel("data/trash_wheel_collection_data.xlsx", 
             sheet = "Gwynns Falls Trash Wheel",
             range = "A2:L351") |>
  janitor::clean_names()

trashwheels_combined_df = 
  bind_rows(
    trashwheel_df |>
      mutate(
        trash_wheel = "Mr. Trash Wheel"
      ),
    professor_trashwheel_df |>
      mutate(
        trash_wheel = "Professor Trash Wheel"
      ),
    gwynnda_trashwheel_df |>
      mutate(
        trash_wheel = "Gwynnda"
      )
  )
```

Write a paragraph about these data; you are encouraged to use inline R.
Be sure to note the number of observations in the resulting dataset, and
give examples of key variables. For available data, what was the total
weight of trash collected by Professor Trash Wheel? What was the total
number of cigarette butts collected by Gwynnda in June of 2022?

Each of the trash_wheel datasets contain information on the amounts of
trash that each trashwheel collected,in weight and volume, when it was
collected, which dumpster it was collected from, and information on the
amounts of specific types of trash it collected (like plastic bottles,
wrappers, sports balls, etc). Each data set also notes how many homes
were powered by the trashwheel that month. The combined dataset,
trashwheels_combined_df contains 1188 observations collected from three
distinct trash wheels: Mr. Trash Wheel, Professor Trash Wheel, and
Gwynnda. Key variables in the combined dataset include the trashwheel
responsible for each row, the weight of trash collected (in tons),
counts of specific items such as cigarette butts, sports balls, and
plastics, as well as time variables like month and year. Professor Trash
Wheel collected a total of 282.26 tons of trash. Meanwhile, Gwynnda
collected 1.812^{4} cigarette butts in June 2022 alone.

## Problem 3

Create a single, well-organized dataset with all the information from
zips and zori datasets, and combine them.

``` r
zips_df = 
  read_csv("data/Zip Codes.csv", na = c("NA", ".", "")) |>
  janitor::clean_names() 
```

    ## Rows: 322 Columns: 7
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (4): County, County Code, File Date, Neighborhood
    ## dbl (3): State FIPS, County FIPS, ZipCode
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
zori_df = 
  read_csv("data/Zip_zori_uc_sfrcondomfr_sm_month_NYC.csv", na = c("NA", ".", "")) |>
  janitor::clean_names() |>
  pivot_longer(cols = x2015_01_31:x2024_08_31,
               names_to = "date",
               values_to = "zori") |>
  mutate(region_name = as.numeric(region_name)) |>
  mutate(county_name = str_remove(county_name," County")) |>
  rename(county = county_name) |>
  rename(zip_code = region_name)
```

    ## Rows: 149 Columns: 125
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr   (6): RegionType, StateName, State, City, Metro, CountyName
    ## dbl (119): RegionID, SizeRank, RegionName, 2015-01-31, 2015-02-28, 2015-03-3...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
joined_zipcode_data =  
  left_join(zips_df, zori_df, by = c("zip_code", "county")) |>
  select(-state, -city, -metro, -region_type, -state_name, -state_fips, -file_date)
```

Briefly describe the resulting tidy dataset. How many total observations
exist? How many unique ZIP codes are included, and how many unique
neighborhoods?

The resulting tidy dataset, joined_zipcode_df, is the zillow data
(zori_df) left joined onto the zipcode data (zips_df) with all the
redundant columns (which all had the same value for every row, like
state, city, metro and so on) removed. Since there were some repeated
zip codes for two different counties, I left joined keeping in mind both
zip code and county instead of zip code alone. Some of the variables in
this data set include the zip code, the neighborhood, the county code,
and the zori data for each case. Total number of observations are 17342.
Number of unique ZIP codes are 320. Number of unique neighborhoods are
43.

Which ZIP codes appear in the ZIP code dataset but not in the Zillow
Rental Price dataset? Using a few illustrative examples discuss why
these ZIP codes might be excluded from the Zillow dataset.

Missing zips (all values in the df, but compressed form showed here for
viewing reasons. )

``` r
missing_zips = zips_df |>
  anti_join(zori_df, by = "zip_code") |>
  select(zip_code)
```

Missing zips are : c(10464, 10474, 10475, 10499, 10550, 10704, 10705,
10803, 11202, 11224, 11239, 11241, 11242, 11243, 11245, 11247, 11251,
11252, 11256, 10008, 10020, 10041, 10043, 10045, 10047, 10048, 10055,
10072, 10080, 10081, 10082, 10087, 10101, 10102, 10103, 10104, 10105,
10106, 10107, 10108, 10109, 10110, 10111, 10112, 10113, 10114, 10115,
10116, 10117, 10118, 10119, 10120, 10121, 10122, 10123, 10124, 10125,
10126, 10129, 10130, 10131, 10132, 10133, 10138, 10149, 10150, 10151,
10152, 10153, 10154, 10155, 10156, 10157, 10158, 10159, 10160, 10161,
10163, 10164, 10165, 10166, 10167, 10168, 10169, 10170, 10171, 10172,
10173, 10174, 10175, 10176, 10177, 10178, 10179, 10185, 10197, 10199,
10213, 10242, 10249, 10256, 10259, 10260, 10261, 10265, 10268, 10269,
10270, 10271, 10272, 10273, 10274, 10275, 10276, 10277, 10278, 10279,
10281, 10285, 10286, 10292, 11001, 11004, 11005, 11040, 11096, 11351,
11352, 11359, 11362, 11363, 11371, 11380, 11381, 11386, 11405, 11411,
11412, 11413, 11414, 11416, 11417, 11419, 11420, 11421, 11422, 11423,
11424, 11425, 11427, 11428, 11429, 11430, 11431, 11433, 11436, 11439,
11451, 11499, 11559, 11580, 11690, 11694, 11695, 11697, 10302, 10307,
10309, 10310, 10311, 10313)

- 10278 - Government building, Jacob K Javits building has its own zip
  code, so no residential properties there for zillow.
- 10118 Empire state building has its own zip code.
- 10111: Rockefeller Center has it’s own zip code.

Some business/ government buildings or financial areas have their own
zip codes in New York, so there cannot be residential properties there
and therefore they wouldn’t show up on Zillow.

Make a table that shows the 10 ZIP codes (along with the borough and
neighborhood) with largest drop in price from January 2020 to 2021.
Comment.

``` r
january_data <- joined_zipcode_data |>
  filter(date == "x2020_01_31" | date == "x2021_01_31") |>
  select(county, zip_code, neighborhood, date, zori) |>
  pivot_wider(names_from = date, values_from = zori, 
              names_prefix = "zori_") |>
  filter(!is.na(zori_x2020_01_31) & !is.na(zori_x2021_01_31)) |>
  mutate(price_change = zori_x2021_01_31 - zori_x2020_01_31) |>
  arrange(price_change)

top_ten_changes <- january_data |>
  slice(1:10) |>
  rename(
    zori_2020 = zori_x2020_01_31,
    zori_2021 = zori_x2021_01_31
  ) |>
  select(zip_code, county, neighborhood, zori_2020, zori_2021, price_change)

kable(top_ten_changes)
```

| zip_code | county | neighborhood | zori_2020 | zori_2021 | price_change |
|---:|:---|:---|---:|---:|---:|
| 10007 | New York | Lower Manhattan | 6334.211 | 5421.614 | -912.5966 |
| 10069 | New York | NA | 4623.042 | 3874.918 | -748.1245 |
| 10009 | New York | Lower East Side | 3406.442 | 2692.187 | -714.2550 |
| 10016 | New York | Gramercy Park and Murray Hill | 3731.135 | 3019.431 | -711.7045 |
| 10001 | New York | Chelsea and Clinton | 4108.098 | 3397.648 | -710.4499 |
| 10002 | New York | Lower East Side | 3645.416 | 2935.113 | -710.3028 |
| 10004 | New York | Lower Manhattan | 3149.658 | 2443.697 | -705.9608 |
| 10038 | New York | Lower Manhattan | 3573.201 | 2875.616 | -697.5853 |
| 10012 | New York | Greenwich Village and Soho | 3628.566 | 2942.344 | -686.2218 |
| 10010 | New York | Gramercy Park and Murray Hill | 3697.284 | 3012.353 | -684.9304 |

Most of these are very expensive areas, but also very dense areas with
very little green space nearby. COVID-19 was probably the worst in these
neighborhoods (the most cases), a lot of people probably didn’t want to
live in a super dense neighborhood with a lot of cases and risk getting
sick, and without an outdoor space to be able to get fresh air, the rent
prices probably seemed very difficult to justify, so the landlords
probably had to shift their prices much lower to adjust for that to try
to convince people to rent in those areas despite covid.
